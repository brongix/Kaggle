{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental imports, agents assessment and submission functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['random', 'negamax']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import os\n",
    "from kaggle_environments import make, evaluate, utils\n",
    "\n",
    "# Create the game environment\n",
    "# Set debug=True to see the errors if your agent refuses to run\n",
    "env = make(\"connectx\", debug=True)\n",
    "\n",
    "# List of available default agents\n",
    "print(list(env.agents))\n",
    "\n",
    "\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n",
    "    \n",
    "\n",
    "\n",
    "def write_agent_to_file(function, file):\n",
    "    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n",
    "        f.write(inspect.getsource(function))\n",
    "        print(function, \"written to\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading previously developed agents - Classic Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submission_test import my_agent_test as level_two_agent\n",
    "from submission_one_step_agent import my_agent_one_step_5_heuristic as one_step_agent\n",
    "from submission_alphabeta_agent_with_improved_heuristics_three_step import alphabeta_agent_with_improved_heuristics as three_step_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.0\n",
      "Agent 2 Win Percentage: 1.0\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n",
      "0.9419407844543457\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "get_win_percentages(level_two_agent, one_step_agent, n_rounds=1)\n",
    "delta = time.time() - t\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 1.0\n",
      "Agent 2 Win Percentage: 0.0\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n",
      "9.665860891342163\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "get_win_percentages(three_step_agent, one_step_agent, n_rounds=1)\n",
    "delta = time.time() - t\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install 'tensorflow==1.15.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2=\"random\"):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym(agent2='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install cmake openmpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stable-baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.0'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stable_baselines\n",
    "\n",
    "stable_baselines.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"ppo/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import PPO1 \n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "\n",
    "# Neural network for predicting action values\n",
    "def modified_cnn(scaled_images, **kwargs):\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n",
    "        \n",
    "# Initialize agent\n",
    "model = PPO1(CustomCnnPolicy, vec_env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343.3762309551239\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Train agent\n",
    "t = time.time()\n",
    "model.learn(total_timesteps=100_000)\n",
    "delta = time.time() - t\n",
    "print(delta)\n",
    "\n",
    "# Plot cumulative reward\n",
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.rolling(window=1000).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent1(obs, config):\n",
    "    #with open('history', mode='a') as file:\n",
    "    #file.write(f'\\n    data = {learner.policy._get_data()}\\n')\n",
    "        #file.write(f'    obs = {obs}\\n config={config}')\n",
    "    # Use the best model to select a column\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.9\n",
      "Agent 2 Win Percentage: 0.1\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 0\n",
      "7.336079120635986\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "get_win_percentages('negamax', agent1, n_rounds=10)\n",
    "delta = time.time() - t\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = model.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model/c1/w:0', 'model/c1/b:0', 'model/c2/w:0', 'model/c2/b:0', 'model/fc1/w:0', 'model/fc1/b:0', 'model/vf/w:0', 'model/vf/b:0', 'model/pi/w:0', 'model/pi/b:0', 'model/q/w:0', 'model/q/b:0'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting RL_test_submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile RL_test_submission.py\n",
    "def RL_test_agent(obs, config):\n",
    "    \n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from stable_baselines import PPO1 \n",
    "    from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "    from stable_baselines.common.policies import CnnPolicy\n",
    "    from kaggle_environments import evaluate, make, utils\n",
    "    from gym import spaces\n",
    "    array = np.array\n",
    "    float32 = np.float32\n",
    "\n",
    "    class ConnectFourGym:\n",
    "        def __init__(self, agent2=\"random\"):\n",
    "            ks_env = make(\"connectx\", debug=True)\n",
    "            self.env = ks_env.train([None, agent2])\n",
    "            self.rows = ks_env.configuration.rows\n",
    "            self.columns = ks_env.configuration.columns\n",
    "            # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "            self.action_space = spaces.Discrete(self.columns)\n",
    "            self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                                shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "            # Tuple corresponding to the min and max possible rewards\n",
    "            self.reward_range = (-10, 1)\n",
    "            # StableBaselines throws error if these are not defined\n",
    "            self.spec = None\n",
    "            self.metadata = None\n",
    "        def reset(self):\n",
    "            self.obs = self.env.reset()\n",
    "            return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "        def change_reward(self, old_reward, done):\n",
    "            if old_reward == 1: # The agent won the game\n",
    "                return 1\n",
    "            elif done: # The opponent won the game\n",
    "                return -1\n",
    "            else: # Reward 1/42\n",
    "                return 1/(self.rows*self.columns)\n",
    "        def step(self, action):\n",
    "            # Check if agent's move is valid\n",
    "            is_valid = (self.obs['board'][int(action)] == 0)\n",
    "            if is_valid: # Play the move\n",
    "                self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "                reward = self.change_reward(old_reward, done)\n",
    "            else: # End the game and penalize agent\n",
    "                reward, done, _ = -10, True, {}\n",
    "            return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "    \n",
    "    # Neural network for predicting action values\n",
    "    def modified_cnn(scaled_images, **kwargs):\n",
    "        activ = tf.nn.relu\n",
    "        layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
    "                             init_scale=np.sqrt(2), **kwargs))\n",
    "        layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
    "                             init_scale=np.sqrt(2), **kwargs))\n",
    "        layer_2 = conv_to_fc(layer_2)\n",
    "        return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "\n",
    "    class CustomCnnPolicy(CnnPolicy):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "agent_path = 'RL_test_submission.py'\n",
    "\n",
    "state_dict = model.get_parameters()\n",
    "\n",
    "state_dict = {\n",
    "    'model/c1/w:0': state_dict['model/c1/w:0'],\n",
    "    'model/c1/b:0': state_dict['model/c1/b:0'],\n",
    "    'model/c2/w:0': state_dict['model/c2/w:0'],\n",
    "    'model/c2/b:0': state_dict['model/c2/b:0'],\n",
    "    'model/fc1/w:0': state_dict['model/fc1/w:0'],\n",
    "    'model/fc1/b:0': state_dict['model/fc1/b:0'],\n",
    "    'model/vf/w:0': state_dict['model/vf/w:0'],\n",
    "    'model/vf/b:0': state_dict['model/vf/b:0'],\n",
    "    'model/pi/w:0': state_dict['model/pi/w:0'],\n",
    "    'model/pi/b:0': state_dict['model/pi/b:0'],\n",
    "    'model/q/w:0': state_dict['model/q/w:0'],\n",
    "    'model/q/b:0': state_dict['model/q/b:0']\n",
    "}\n",
    "\n",
    "with open(agent_path, mode='a') as file:\n",
    "    #file.write(f'\\n    data = {learner.policy._get_data()}\\n')\n",
    "    file.write(f'    state_dict = {state_dict}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to RL_test_submission.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a RL_test_submission.py\n",
    "    \n",
    "    # Create ConnectFour environment\n",
    "    env = ConnectFourGym(agent2='random')\n",
    "\n",
    "    # Initialize agent\n",
    "    model = PPO1(CustomCnnPolicy, env, verbose=0)\n",
    "    \n",
    "    model.load_parameters(state_dict)\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_test_submission import RL_test_agent as RL_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 Win Percentage: 0.0\n",
      "Agent 2 Win Percentage: 0.0\n",
      "Number of Invalid Plays by Agent 1: 0\n",
      "Number of Invalid Plays by Agent 2: 10\n",
      "0.3256559371948242\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "get_win_percentages('random', RL_agent, n_rounds=10)\n",
    "delta = time.time() - t\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_state_dict import get_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters(get_state_dict(), exact_match=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = {'board': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mark': 1}\n",
    "config={'episodeSteps': 1000, 'agentTimeout': 16, 'actTimeout': 8, 'runTimeout': 1200, 'isProduction': False, 'columns': 7, 'rows': 6, 'inarow': 4, 'timeout': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Error: the model does not support input space of type NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-9f627bbebbbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mRL_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Google Drive/My Lab/Kaggle/Intro to Game AI and RL/RL_test_submission.py\u001b[0m in \u001b[0;36mRL_test_agent\u001b[0;34m(obs, config)\u001b[0m\n\u001b[1;32m  76054\u001b[0m        [ 6.47961569e-04, -4.95163898e-04,  2.28087039e-04,\n\u001b[1;32m  76055\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;36m7.09054642e-04\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1.99687318e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.06346502e-04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 76056\u001b[0;31m         -3.15878315e-05],\n\u001b[0m\u001b[1;32m  76057\u001b[0m        [ 6.16785139e-04, -3.40639788e-04, -2.01642033e-04,\n\u001b[1;32m  76058\u001b[0m          \u001b[0;36m1.32965099e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2.05602191e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.39516400e-04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, gamma, timesteps_per_actorbatch, clip_param, entcoeff, optim_epochs, optim_stepsize, optim_batchsize, lam, adam_epsilon, schedule, verbose, tensorboard_log, _init_setup_model, policy_kwargs, full_tensorboard_log, seed, n_cpu_tf_sess)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pretrain_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/ppo1/pposgd_simple.py\u001b[0m in \u001b[0;36msetup_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;31m# Construct network for new policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 self.policy_pi = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n\u001b[0;32m--> 107\u001b[0;31m                                              None, reuse=False, **self.policy_kwargs)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m# Network for old policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/My Lab/Kaggle/Intro to Game AI and RL/RL_test_submission.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mCustomCnnPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCnnPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomCnnPolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_extractor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodified_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse, **_kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         super(CnnPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse,\n\u001b[0;32m--> 601\u001b[0;31m                                         feature_extraction=\"cnn\", **_kwargs)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse, layers, net_arch, act_fun, cnn_extractor, feature_extraction, **kwargs)\u001b[0m\n\u001b[1;32m    538\u001b[0m                  act_fun=tf.tanh, cnn_extractor=nature_cnn, feature_extraction=\"cnn\", **kwargs):\n\u001b[1;32m    539\u001b[0m         super(FeedForwardPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n\u001b[0;32m--> 540\u001b[0;31m                                                 scale=(feature_extraction == \"cnn\"))\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse, scale)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         super(ActorCriticPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=reuse,\n\u001b[0;32m--> 221\u001b[0;31m                                                 scale=scale)\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_proba_dist_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse, scale, obs_phs, add_action_ph)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mobs_phs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processed_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_ph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processed_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_phs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/stable_baselines/common/input.py\u001b[0m in \u001b[0;36mobservation_input\u001b[0;34m(ob_space, batch_size, name, scale)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         raise NotImplementedError(\"Error: the model does not support input space of type {}\".format(\n\u001b[0;32m---> 51\u001b[0;31m             type(ob_space).__name__))\n\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m: Error: the model does not support input space of type NoneType"
     ]
    }
   ],
   "source": [
    "RL_agent(obs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
